{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9f03b6-0436-4d56-9c51-84cfd1569c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ─── Cell 1: Install dependencies ───\n",
    "# !pip install transformers torch langchain langchain-huggingface chromadb langchain_community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfaa4e88-8e06-4e09-aa2a-76b9a29c67c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b2a41b7-7be0-4c98-a8e3-85f3cc67a620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "Device set to use cpu\n",
      "/tmp/ipykernel_10360/3738797258.py:25: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 2: Imports & Pipeline Setup ───\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# 1) Build local HF pipeline\n",
    "model_id = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "hf_pipe   = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    device=-1,            # CPU; set to 0 for GPU\n",
    ")\n",
    "\n",
    "# 2) Wrap in LangChain LLM\n",
    "hf_llm = HuggingFacePipeline(pipeline=hf_pipe)\n",
    "\n",
    "# 3) Embeddings & VectorStore factory\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0737042b-61d5-4c5b-ab05-f704e766bbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ QA chain ready!\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 3: Load a Website and Build the QA Chain ───\n",
    "# Replace with any URL you like\n",
    "test_url = \"https://www.gsmarena.com/\"\n",
    "\n",
    "# 1) Load & split\n",
    "loader    = WebBaseLoader(test_url)\n",
    "docs      = loader.load()\n",
    "\n",
    "# 2) Build vector store\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"test-site\"\n",
    ")\n",
    "\n",
    "# 3) Build RetrievalQA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=hf_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "print(\"✅ QA chain ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d500b374-2c67-4366-bbed-bdb93a505c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ─── Cell 3: Load a Website and Build the QA Chain ───\n",
    "# # Replace with any URL you like\n",
    "# test_url = \"https://www.example.com/\"\n",
    "\n",
    "# # 1) Load & split\n",
    "# loader    = WebBaseLoader(test_url)\n",
    "# docs      = loader.load()\n",
    "\n",
    "# # 2) Build vector store\n",
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=docs,\n",
    "#     embedding=embeddings,\n",
    "#     collection_name=\"test-site\"\n",
    "# )\n",
    "\n",
    "# # 3) Build RetrievalQA chain\n",
    "# qa = RetrievalQA.from_chain_type(\n",
    "#     llm=hf_llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=vectordb.as_retriever()\n",
    "# )\n",
    "\n",
    "# print(\"✅ QA chain ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c060a00-b7a7-466a-a525-7f42f25730ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10360/3907341172.py:9: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = qa.run(q)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1791 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the top news in this website\n",
      "A: Apple and Aston Martin announce CarPlay Ultra - a next-generation infotainment system\n",
      "------------------------------------------------------------\n",
      "Q: Does it mention any contact information?\n",
      "A: GSMArena.com\n",
      "------------------------------------------------------------\n",
      "Q: Summarize the first paragraph.\n",
      "A: The following is a list of the best smartphones of the year.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 4: Ask It Questions ───\n",
    "queries = [\n",
    "    \"What is the top news in this website\",\n",
    "    \"Does it mention any contact information?\",\n",
    "    \"Summarize the first paragraph.\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    answer = qa.run(q)\n",
    "    print(f\"Q: {q}\\nA: {answer}\\n\" + \"-\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bbdab0-cafb-43f8-9c28-faa4e41fa7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
